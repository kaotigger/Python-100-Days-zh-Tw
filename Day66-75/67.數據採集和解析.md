## 數據採集和解析

通過[《網絡爬蟲和相關工具》](./01.網絡爬蟲和相關工具.md)一文，我們已經了解到了開發一個爬蟲需要做的工作以及一些常見的問題，至此我們可以對爬蟲開發需要做的工作以及相關的技術做一個簡單的匯總，這其中可能會有一些我們之前沒有使用過的第三方庫，不過別擔心，這些內容我們稍後都會一一講到。

1. 下載數據 - urllib / requests / aiohttp。
2. 解析數據 - re / lxml / beautifulsoup4（bs4）/ pyquery。
3. 緩存和持久化 - pymysql / sqlalchemy / peewee/ redis / pymongo。
4. 生成數字簽名 - hashlib。
5. 序列化和壓縮 - pickle / json / zlib。
6. 調度器 - 進程（multiprocessing） / 線程（threading） / 協程（coroutine）。

### HTML頁面分析

```HTML
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>首頁</title>
    </head>
    <body>
        <h1>Hello, world!</h1>
        <p>這是一個神奇的網站！ </p>
        <hr>
        <div>
            <h2>這是一個例子程序</h2>
            <p>靜夜思</p>
            <p class="foo">床前明月光</p>
            <p id="bar">疑似地上霜</p>
            <p class="foo">舉頭望明月</p>
            <div><a href="http://www.baidu.com"><p>低頭思故鄉</p></a></div>
        </div>
        <a class="foo" href="http://www.qq.com">騰訊網</a>
        <img src="./img/pretty-girl.png" alt="美女">
        <img src="./img/hellokitty.png" alt="凱蒂貓">
        <img src="/static/img/pretty-girl.png" alt="美女">
        <table>
            <tr>
                <th>姓名</th>
                <th>上場時間</th>
                <th>得分</th>
                <th>籃板</th>
                <th>助攻</th>
            </tr>
        </table>
    </body>
</html>
```

如果你對上面的代碼並不感到陌生，那麼你一定知道HTML頁面通常由三部分構成，分別是用來承載內容的Tag（標籤）、負責渲染頁面的CSS（層疊樣式表）以及控制交互式行為的JavaScript。通常，我們可以在瀏覽器的右鍵菜單中通過“查看網頁源代碼”的方式獲取網頁的代碼並了解頁面的結構；當然，我們也可以通過瀏覽器提供的開發人員工具來了解網頁更多的信息。

#### 使用requests獲取頁面

1. GET請求和POST請求。

   ```Python
   
   ```

2. URL參數和請求頭。

   ```Python
   
   ```

3. 複雜的POST請求（文件上傳）。

   ```Python
   
   ```

4. 操作Cookie。

   ```Python
   
   ```

5. 設置代理服務器。

   ```Python
   
   ```

> 說明：關於requests的詳細用法可以參考它的[官方文檔](http://docs.python-requests.org/zh_CN/latest/user/quickstart.html)。

### 四種採集方式

#### 四種採集方式的比較

| 抓取方法 | 速度 | 使用難度 | 備註 |
| ------------- | ------------------------- | -------- | ------------------------------------------ |
| 正則表達式 | 快 | 困難 | 常用正則表達式<br>在線正則表達式測試 |
| lxml | 快 | 一般 | 需要安裝C語言依賴庫<br>唯一支持XML的解析器 |
| BeautifulSoup | 較快/較慢（取決於解析器） | 簡單 | |
| PyQuery | 較快 | 簡單 | Python版的jQu​​ery |

> 說明：BeautifulSoup可選的解析器包括：Python標準庫（html.parser）、lxml的HTML解析器、lxml的XML解析器和html5lib。

#### 使用正則表達式

如果你對正則表達式沒有任何的概念，那麼推薦先閱讀[《正則表達式30分鐘入門教程》]()，然後再閱讀我們之前講解在Python中如何使用正則表達式一文。

#### 使用XPath和Lxml



#### BeautifulSoup的使用

BeautifulSoup是一個可以從HTML或XML文件中提取數據的Python庫。它能夠通過你喜歡的轉換器實現慣用的文檔導航、查找、修改文檔的方式。

1. 遍歷文檔樹
   - 獲取標籤
   - 獲取標籤屬性
   - 獲取標籤內容
   - 獲取子（孫）節點
   - 獲取父節點/祖先節點
   - 獲取兄弟節點
2. 搜索樹節點
   - find / find_all：字符串、正則表達式、列表、True、函數或Lambda。
   - select_one / select：CSS選擇器

> 說明：更多內容可以參考BeautifulSoup的[官方文檔](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)。

#### PyQuery的使用

pyquery相當於jQuery的Python實現，可以用於解析HTML網頁。



### 實例 - 獲取知乎發現上的問題鏈接

```Python
from urllib.parse import urljoin

import re
import requests

from bs4 import BeautifulSoup


def main():
    headers = {'user-agent': 'Baiduspider'}
    proxies = {
        'http': 'http://122.114.31.177:808'
    }
    base_url = 'https://www.zhihu.com/'
    seed_url = urljoin(base_url, 'explore')
    resp = requests.get(seed_url,
                        headers=headers,
                        proxies=proxies)
    soup = BeautifulSoup(resp.text, 'lxml')
    href_regex = re.compile(r'^/question')
    link_set = set()
    for a_tag in soup.find_all('a', {'href': href_regex}):
        if 'href' in a_tag.attrs:
            href = a_tag.attrs['href']
            full_url = urljoin(base_url, href)
            link_set.add(full_url)
    print('Total %d question pages found.' % len(link_set))


if __name__ == '__main__':
    main()
```
